import jax
import jax.numpy as jnp
import optax
import rlax
import flax
import tqdm
import jax.numpy as jnp
import numpy as np
import time

from flax.training.train_state import TrainState
from src.models.actor_critic import *
from typing import Callable,Tuple
from src.agents.base_agent import BaseAgent



class PPOAgent(BaseAgent):

    def __init__(self,train_envs,eval_env,repr_model_fn:Callable,seq_model_fn:Tuple[Callable,Callable],
                        actor_fn:Callable,critic_fn:Callable,optimizer:optax.GradientTransformation,
                         num_steps=128, anneal_lr=True, gamma=0.99,
                        gae_lambda=0.95, num_minibatches=4, update_epochs=4, norm_adv=True,
                        clip_coef=0.1, ent_coef=0.01, vf_coef=0.5, max_grad_norm=0.5,
                        target_kl=None) -> None:

        super(PPOAgent,self).__init__(train_envs=train_envs,eval_env=eval_env,rollout_len=num_steps,repr_model_fn=repr_model_fn,seq_model_fn=seq_model_fn,
                        actor_fn=actor_fn,critic_fn=critic_fn,use_gumbel_sampling=True)
        
        self.optimizer=optimizer
        self.num_envs = self.env.num_envs
        self.anneal_lr = anneal_lr
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.num_minibatches = num_minibatches
        self.update_epochs = update_epochs
        self.norm_adv = norm_adv
        self.clip_coef = clip_coef
        self.ent_coef = ent_coef
        self.vf_coef = vf_coef
        self.max_grad_norm = max_grad_norm
        self.target_kl = target_kl

    
        @jax.jit
        def update_ppo(
            params,optimizer_state,random_key,h_tickminus1,
            data_batch
        ):
            Glambda_fn=jax.vmap(rlax.lambda_returns)

            observations,actions,rewards,terminations,critic_preds,actor_preds=data_batch['observations'],data_batch['actions'], \
                                            data_batch['rewards'],data_batch['terminations'],data_batch['critic_preds'],data_batch['actor_preds']
            gammas=self.gamma*(1-terminations)
            lambdas=self.gae_lambda*jnp.ones(self.num_envs)
            #Calculate Lamba for timesteps G_{tick} - G_{tick+rollout_len}
            #rewards, gammas, lambdas values at timesteps {tick+1} - {tick+rollout_len+1}
            Glambdas=Glambda_fn(rewards[:,1:],gammas[:,1:],
                              critic_preds[:,1:],lambdas)
            #Calculate the advantages using timesteps {tick} - {tick+rollout_len}
            advantages=Glambdas-critic_preds[:,:-1]
            #Calculate log probs shape (num_envs*rollout_len,num_actions)
            B,T=actions.shape
            logprobs=jax.nn.log_softmax(actor_preds).reshape(B*T,-1)
            logprobs=logprobs[jnp.arange(B*T),actions.reshape(-1)].reshape(B,T)
            #Calculate log probs of actions taken√ü
            

            def ppo_loss(params, random_key, mb_observations, mb_actions,mb_terminations,
                            mb_logp, mb_advantages, mb_returns,mb_h_tickminus1):
                logits_new,values_new,_=self.actor_critic_fn(random_key,params,mb_observations,mb_terminations,
                                                             mb_h_tickminus1)
                #newlogprob, entropy, newvalue = get_action_and_value2(random_key,params, x, a)
                B,T=mb_actions.shape
                newlogprobs=jax.nn.log_softmax(logits_new).reshape(B*T,-1)
                newlogprobs=newlogprobs[jnp.arange(B*T),mb_actions.reshape(-1)].reshape(B,T)
                # normalize the logits https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/
                logits_new = logits_new - jax.scipy.special.logsumexp(logits_new, axis=-1, keepdims=True)
                logits_new = logits_new.clip(min=jnp.finfo(logits_new.dtype).min)
                p_log_p = logits_new * jax.nn.softmax(logits_new)
                entropy = -p_log_p.sum(-1)

                logratio = newlogprobs - mb_logp
                ratio = jnp.exp(logratio)
                approx_kl = ((ratio - 1) - logratio).mean()

                if self.norm_adv:
                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)

                # Policy loss
                pg_loss1 = -mb_advantages * ratio
                pg_loss2 = -mb_advantages * jnp.clip(ratio, 1 - self.clip_coef, 1 + self.clip_coef)
                pg_loss = jnp.maximum(pg_loss1, pg_loss2).mean()

                # Value loss
                v_loss = 0.5 * ((values_new - mb_returns) ** 2).mean()

                entropy_loss = entropy.mean()
                loss = pg_loss - self.ent_coef * entropy_loss + v_loss * self.vf_coef
                return loss, (pg_loss, v_loss, entropy_loss, jax.lax.stop_gradient(approx_kl))

            ppo_loss_grad_fn = jax.value_and_grad(ppo_loss, has_aux=True)

            envsperbatch = self.num_envs // self.num_minibatches
            env_inds=jnp.arange(self.num_envs,dtype=int)

            #Use observations tick to tick+rollout_len
            observations=observations[:,:-1]
            terminations=terminations[:,:-1]
            for _ in range(self.update_epochs):
                shuffle_key, model_key,random_key = jax.random.split(random_key,3)
                env_inds=jax.random.shuffle(shuffle_key,env_inds)
                for start in range(0, self.num_envs, self.num_minibatches):
                    end = start + envsperbatch
                    mbenvinds = env_inds[start:end]
                    model_key, _ = jax.random.split(model_key)
                    mb_h_tickminus1=jax.tree_map(lambda x:x[mbenvinds],h_tickminus1)
                    (loss, (pg_loss, v_loss, entropy_loss, approx_kl)), grads = ppo_loss_grad_fn(
                        params,
                        model_key,
                        observations[mbenvinds],
                        actions[mbenvinds],
                        terminations[mbenvinds],
                        logprobs[mbenvinds],
                        advantages[mbenvinds],
                        Glambdas[mbenvinds],
                        mb_h_tickminus1
                    )
                    updates,optimizer_state=self.optimizer.update(grads,optimizer_state)
                    params=optax.apply_updates(params,updates)
            return (loss, pg_loss, v_loss, entropy_loss, approx_kl),params, optimizer_state
        self.update_ppo = update_ppo

        
    def reset(self,params_key,random_key):
        super(PPOAgent,self).reset(params_key,random_key)
        self.optimizer_state=self.optimizer.init(self.params)

    def step(self,random_key):
        #Unroll actor for rollout_len steps
        h_tickminus1=self.h_tickminus1.copy()
        #Need to remove values
        unroll_key,update_key=jax.random.split(random_key)
        databatch=self.unroll_actors(unroll_key)
        databatch=vars(databatch)
        infos=databatch.pop('infos')
        (loss, pg_loss, v_loss, entropy_loss, approx_kl),self.params, self.optimizer_state=self.update_ppo(self.params,
                                self.optimizer_state,update_key,h_tickminus1,databatch)
        
        rewards=databatch['rewards']
        return (loss,(v_loss,entropy_loss,pg_loss,rewards),infos) #Will clean this up later




